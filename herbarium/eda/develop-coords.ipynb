{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Make the code that generates the bounding boxes, clusters and centroids into a .py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_329770/903731494.py:2: DtypeWarning: Columns (49,56,57,59,60,61,63,64,65,66,67,68,73,74,77,78,79,83,89,91,92,95,97,98,99,100,101,102,103,105,106,107,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,135,137,138,140,141,142,143,144,145,146,147,149,150,151,152,153,154,155,160,162,163,164,167,168,171,173,174,177,178,182,183,184,185,186,189,190,191,192,193,194,195,196,197,198,199,200,204,208,209,210,211,220,221) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  occ = pd.read_csv(\"/data/cher/universe7/herbarium/data/MO-herbarium/occurrence.txt\", sep=\"\\t\", on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "# Get Herbarium data. Find some candidate locations.\n",
    "occ = pd.read_csv(\"/data/cher/universe7/herbarium/data/MO-herbarium/occurrence.txt\", sep=\"\\t\", on_bad_lines='skip')\n",
    "occ = occ[[\n",
    "    'gbifID', 'occurrenceID', # Joining Keys\n",
    "    'speciesKey', 'species', # species\n",
    "    'year', 'month', 'day', # date\n",
    "    'habitat', 'locality', # descriptive text. I think this is most of the route / habitat info we would want. Looks like already OCR processed?\n",
    "    'countryCode', 'stateProvince', 'county', 'municipality', # administrative\n",
    "    'georeferenceSources','hasCoordinate', 'hasGeospatialIssues' ,'decimalLatitude', 'decimalLongitude']] # geospatial\n",
    "    # 'level0Gid', 'level0Name', 'level1Gid', 'level1Name', 'level2Gid', 'level2Name', 'level3Gid', 'level3Name',]] # Has NAs. Above is more reliable.\n",
    "\n",
    "habitat_info = occ[~occ['habitat'].isna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random location\n",
    "random_row = habitat_info[~habitat_info['habitat'].isna()].iloc[-4]\n",
    "lat, lon = random_row['decimalLatitude'], random_row['decimalLongitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAIP \n",
    "- Only 3 band RGB through WMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/data/cher/universe7/herbarium/data/naip/1.tiff',\n",
       " <http.client.HTTPMessage at 0x7f105f5980e0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "image_url = f\"https://basemap.nationalmap.gov/arcgis/services/USGSImageryOnly/MapServer/WMSServer?service=WMS&version=1.1.1&request=GetMap&layers=0&styles=&width=256&height=256&srs=EPSG:4326&bbox={lon-0.005},{lat-0.005},{lon+0.005},{lat+0.005}&format=image/png\"\n",
    "out_file = f'/data/cher/universe7/herbarium/data/naip/1.png'\n",
    "\n",
    "urllib.request.urlretrieve(image_url, out_file)\n",
    "\n",
    "with rasterio.open(out_file) as src:\n",
    "    # Get the number of bands\n",
    "    num_bands = src.count\n",
    "    print(f'The TIFF file has {num_bands} bands.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentinel2\n",
    "- Create csv file to push into CVGlobal \n",
    "    1) Cluster points state-by-state using DBSCAN into max areas of 512 x 512 meters --> cell\n",
    "    2) Find center point of each cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, box\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \n",
    "    data = data[~data['stateProvince'].isna()]\n",
    "\n",
    "    data = data[\n",
    "        pd.to_numeric(data['decimalLatitude'], errors='coerce').notnull() &\n",
    "        pd.to_numeric(data['decimalLongitude'], errors='coerce').notnull()\n",
    "    ]\n",
    "    data['decimalLatitude'] = data['decimalLatitude'].astype(float)\n",
    "    data['decimalLongitude'] = data['decimalLongitude'].astype(float)\n",
    "\n",
    "    data['geometry'] = data.apply(\n",
    "        lambda row: Point(row['decimalLongitude'], row['decimalLatitude']), axis=1\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_356390/1733575631.py:2: DtypeWarning:\n",
      "\n",
      "Columns (49,56,57,59,60,61,63,64,65,66,67,68,73,74,77,78,79,83,89,91,92,95,97,98,99,100,101,102,103,105,106,107,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,135,137,138,140,141,142,143,144,145,146,147,149,150,151,152,153,154,155,160,162,163,164,167,168,171,173,174,177,178,182,183,184,185,186,189,190,191,192,193,194,195,196,197,198,199,200,204,208,209,210,211,220,221) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Herbarium data.\n",
    "habitat_info = pd.read_csv(\"/data/cher/universe7/herbarium/data/MO-herbarium/occurrence.txt\", sep=\"\\t\", on_bad_lines='skip')\n",
    "habitat_info = habitat_info[[ 'occurrenceID', 'habitat' , 'stateProvince', 'decimalLatitude', 'decimalLongitude']]\n",
    "\n",
    "data = habitat_info[~habitat_info['habitat'].isna()].copy()\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "data = clean_data(data[['occurrenceID','stateProvince','decimalLatitude', 'decimalLongitude']].copy())\n",
    "data['geometry'] = data.apply(lambda row: Point(row['decimalLongitude'], row['decimalLatitude']), axis=1)\n",
    "gdf = gpd.GeoDataFrame(data, geometry='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_state_clusters_to_files(input_gdf, radius = 512, output_geojson='./cache/locations/clusters.geojson', output_csv='./cache/locations/state_clusters.csv'):\n",
    "    gdf = input_gdf.copy()\n",
    "    \n",
    "    # Create the output directories if they do not exist\n",
    "    output_geojson_dir = os.path.dirname(output_geojson)\n",
    "    output_csv_dir = os.path.dirname(output_csv)\n",
    "    \n",
    "    if output_geojson_dir and not os.path.exists(output_geojson_dir):\n",
    "        os.makedirs(output_geojson_dir)\n",
    "    \n",
    "    if output_csv_dir and not os.path.exists(output_csv_dir):\n",
    "        os.makedirs(output_csv_dir)\n",
    "\n",
    "    # Initialize empty GeoDataFrame and DataFrame for saving results iteratively\n",
    "    all_bounding_boxes = gpd.GeoDataFrame(columns=['state_cluster', 'lat', 'lon' ,'geometry'], geometry='geometry')\n",
    "    combined_clusters_df = pd.DataFrame()\n",
    "\n",
    "    # Create empty files if they don't exist or overwrite existing ones\n",
    "    combined_clusters_df.to_csv(output_csv, index=False)\n",
    "\n",
    "    # Process each state\n",
    "    states = gdf['stateProvince'].unique()\n",
    "    for state in tqdm(states, desc=\"Processing States\"):\n",
    "        print(state)\n",
    "        gdf_s = gdf[gdf['stateProvince'] == state].copy()\n",
    "\n",
    "        # Convert to numpy array for DBSCAN\n",
    "        coords = np.array(list(zip(gdf_s['decimalLongitude'], gdf_s['decimalLatitude'])))\n",
    "\n",
    "        # Use DBSCAN to group points within 512 meters (approx. 0.0046 degrees)\n",
    "        radius_deg = round(radius / 111320, 4)\n",
    "        db = DBSCAN(eps=radius_deg, min_samples=1).fit(coords)\n",
    "        gdf_s['state_cluster'] = [f\"{state}_{label}\" for label in db.labels_]\n",
    "\n",
    "        # Save occurrence_ID and state_cluster to a CSV iteratively\n",
    "        clusters_df = gdf_s[['occurrenceID', 'state_cluster']]\n",
    "        clusters_df.to_csv(output_csv, mode='a', header=False, index=False)\n",
    "\n",
    "        # Create bounding boxes for each cluster\n",
    "        bounding_boxes = []\n",
    "        for label in gdf_s['state_cluster'].unique():\n",
    "            cluster_points = gdf_s[gdf_s['state_cluster'] == label]\n",
    "            minx, miny = cluster_points.geometry.x.min() - (radius_deg / 2), cluster_points.geometry.y.min() - (radius_deg / 2)\n",
    "            maxx, maxy = cluster_points.geometry.x.max() + (radius_deg /  2), cluster_points.geometry.y.max() + (radius_deg / 2)\n",
    "            bbox = box(minx, miny, maxx, maxy)\n",
    "            centroid = bbox.centroid\n",
    "\n",
    "            bounding_boxes.append({\n",
    "                'state_cluster': label,\n",
    "                'lat': centroid.y,\n",
    "                'lon': centroid.x,\n",
    "                'geometry': bbox,\n",
    "\n",
    "            })\n",
    "            \n",
    "        # Convert bounding boxes to GeoDataFrame and save to GeoJSON iteratively\n",
    "        bbox_gdf = gpd.GeoDataFrame(bounding_boxes)\n",
    "\n",
    "        # If the GeoJSON file doesn't exist, create it\n",
    "        if not os.path.exists(output_geojson):\n",
    "            bbox_gdf.to_file(output_geojson, driver=\"GeoJSON\")\n",
    "        else:\n",
    "            bbox_gdf.to_file(output_geojson, driver=\"GeoJSON\", mode='a')\n",
    "\n",
    "    print(f\"Saved all clusters to {output_geojson}\")\n",
    "    print(f\"Saved occurrence_ID and state_cluster mapping to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:   0%|          | 0/53 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virginia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:   2%|▏         | 1/53 [00:01<00:58,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:   4%|▍         | 2/53 [00:05<02:31,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michigan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:   6%|▌         | 3/53 [00:06<01:38,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:   8%|▊         | 4/53 [00:12<02:52,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arkansas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:   9%|▉         | 5/53 [00:13<02:10,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nevada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  11%|█▏        | 6/53 [00:25<04:43,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kentucky\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  13%|█▎        | 7/53 [00:26<03:13,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Massachusetts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  15%|█▌        | 8/53 [00:26<02:16,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utah\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  17%|█▋        | 9/53 [00:44<05:41,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alaska\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  19%|█▉        | 10/53 [00:47<04:18,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idaho\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  21%|██        | 11/53 [00:51<03:56,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Jersey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  23%|██▎       | 12/53 [00:52<02:54,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "North Carolina\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  25%|██▍       | 13/53 [00:56<02:38,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  26%|██▋       | 14/53 [00:57<02:03,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pennsylvania\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  28%|██▊       | 15/53 [00:59<01:47,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florida\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  30%|███       | 16/53 [01:01<01:32,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "West Virginia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  32%|███▏      | 17/53 [01:02<01:10,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  34%|███▍      | 18/53 [01:02<00:57,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delaware\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  36%|███▌      | 19/53 [01:03<00:43,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyoming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  38%|███▊      | 20/53 [01:05<00:52,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missouri\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  40%|███▉      | 21/53 [01:07<00:51,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alabama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  42%|████▏     | 22/53 [01:08<00:43,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maryland\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  43%|████▎     | 23/53 [01:09<00:38,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oklahoma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  45%|████▌     | 24/53 [01:09<00:32,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecticut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  47%|████▋     | 25/53 [01:10<00:26,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mississippi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  49%|████▉     | 26/53 [01:11<00:22,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  51%|█████     | 27/53 [01:13<00:33,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Georgia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  53%|█████▎    | 28/53 [01:14<00:31,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tennessee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  55%|█████▍    | 29/53 [01:15<00:28,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ohio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  57%|█████▋    | 30/53 [01:16<00:22,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indiana\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  58%|█████▊    | 31/53 [01:17<00:20,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "South Dakota\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  60%|██████    | 32/53 [01:17<00:18,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "South Carolina\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  62%|██████▏   | 33/53 [01:18<00:17,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kansas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  64%|██████▍   | 34/53 [01:20<00:20,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iowa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  66%|██████▌   | 35/53 [01:20<00:16,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vermont\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  68%|██████▊   | 36/53 [01:21<00:14,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wisconsin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  70%|██████▉   | 37/53 [01:22<00:13,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colorado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  72%|███████▏  | 38/53 [01:24<00:18,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Mexico\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  74%|███████▎  | 39/53 [01:25<00:18,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arizona\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  75%|███████▌  | 40/53 [01:28<00:22,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Louisiana\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  77%|███████▋  | 41/53 [01:29<00:16,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oregon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  79%|███████▉  | 42/53 [01:31<00:19,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nebraska\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  81%|████████  | 43/53 [01:32<00:15,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rhode Island\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  83%|████████▎ | 44/53 [01:33<00:11,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illinois\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  85%|████████▍ | 45/53 [01:34<00:08,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "North Dakota\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  87%|████████▋ | 46/53 [01:34<00:06,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montana\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  89%|████████▊ | 47/53 [01:35<00:06,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Hampshire\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  91%|█████████ | 48/53 [01:36<00:04,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minnesota\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  92%|█████████▏| 49/53 [01:37<00:03,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "District of Columbia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  94%|█████████▍| 50/53 [01:37<00:02,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wyoming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  96%|█████████▌| 51/53 [01:38<00:01,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New jersey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States:  98%|█████████▊| 52/53 [01:38<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oklahoma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing States: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all clusters to ./cache/locations/clusters.geojson\n",
      "Saved occurrence_ID and state_cluster mapping to ./cache/locations/state_clusters.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process clusters and save results to GeoJSON and CSV files\n",
    "# Not set up to check whether already exists or not. Pretty quick, so re-run with full occurrences as needed.\n",
    "process_state_clusters_to_files(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "clusters = gpd.read_file('./cache/locations/clusters.geojson')\n",
    "cluster_assignments = pd.read_csv('./cache/locations/state_clusters.csv', header = None)\n",
    "cluster_assignments.columns = ['occurrenceID', 'state_cluster']\n",
    "\n",
    "# Are places assigned to the same cluster within 512 meters of one another?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use grid cells across US that have occurrences for query of images.\n",
    "# sentinal2_key.csv -- Source - gbif, key - numeric_key, lat, lon  \n",
    "# obs_id2key .csv -- [Source, obs_id, key, bbox_bounds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurrences: 12654\n",
      "Satellite images: 4798\n",
      "Descriptions per image 2.634014172571905\n"
     ]
    }
   ],
   "source": [
    "# ## Inefficient method -- Grid across state. point-in-polygon.\n",
    "    # 1) Create equal spatial grid of CA (512M x 512M)\n",
    "    # 2) Find center point of each cell\n",
    "    # 3) Remove grids outside CA\n",
    "    # 4) Calculate # of cells with observation\n",
    "    # 5) Calculate # of observations per cell\n",
    "\n",
    "# # California ~4 min\n",
    "\n",
    "# ## Inefficient method\n",
    "# import geopandas as gpd\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from shapely.geometry import box, Point\n",
    "# from joblib import Parallel, delayed\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def build_grid(cell_size_m, bbox):\n",
    "#     # Approximate conversion factors\n",
    "#     deg_per_meter_lat = 1 / 111000  # degrees per meter for latitude\n",
    "#     deg_per_meter_lon = 1 / (111000 * np.cos(np.radians((bbox[1] + bbox[3]) / 2)))  # degrees per meter for longitude\n",
    "\n",
    "#     # Calculate the number of rows and columns\n",
    "#     lat_range = bbox[3] - bbox[1]  # degrees of latitude\n",
    "#     lon_range = bbox[2] - bbox[0]  # degrees of longitude\n",
    "\n",
    "#     nrows = int(np.ceil(lat_range / (cell_size_m * deg_per_meter_lat)))\n",
    "#     ncols = int(np.ceil(lon_range / (cell_size_m * deg_per_meter_lon)))\n",
    "\n",
    "#     # Use tqdm to create a progress bar\n",
    "#     tasks = [(i, j, cell_size_m, deg_per_meter_lat, deg_per_meter_lon, bbox) for i in range(nrows) for j in range(ncols)]\n",
    "#     with Parallel(n_jobs=-1) as parallel:\n",
    "#         grid_polygons = list(tqdm(parallel(delayed(create_cell)(*task) for task in tasks), total=len(tasks)))\n",
    "\n",
    "#     return grid_polygons\n",
    "\n",
    "# def create_cell(i, j, cell_size_m, deg_per_meter_lat, deg_per_meter_lon, bbox):\n",
    "#     # Calculate the corners of the cell\n",
    "#     min_lat = bbox[1] + i * cell_size_m * deg_per_meter_lat\n",
    "#     max_lat = bbox[1] + (i + 1) * cell_size_m * deg_per_meter_lat\n",
    "#     min_lon = bbox[0] + j * cell_size_m * deg_per_meter_lon\n",
    "#     max_lon = bbox[0] + (j + 1) * cell_size_m * deg_per_meter_lon\n",
    "\n",
    "#     # Create a polygon for the cell\n",
    "#     return box(min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "# california = gpd.read_file('/data/cher/universe7/herbarium/data/California_State_Boundary.geojson')\n",
    "# grid_polygons = build_grid(cell_size_m = 1024, bbox = california.total_bounds)\n",
    "\n",
    "# # Create a GeoDataFrame for the grid\n",
    "# grid_gdf = gpd.GeoDataFrame(geometry=grid_polygons)\n",
    "# grid_gdf.set_crs(california.crs, inplace=True)\n",
    "# grid_within_ca = gpd.sjoin(grid_gdf, california, how='inner', predicate='intersects')\n",
    "\n",
    "# # Create a DataFrame with center points of remaining cells\n",
    "# grid_within_ca = gpd.GeoDataFrame(grid_within_ca[['geometry']], geometry='geometry')\n",
    "# grid_centers = grid_within_ca.geometry.apply(lambda geom: geom.centroid)\n",
    "# centroid_points = pd.DataFrame(grid_centers.apply(lambda point: (point.x, point.y)).tolist(), columns=['Longitude', 'Latitude'])\n",
    "\n",
    "# CA_data = habitat_info[habitat_info['stateProvince'] == 'California']\n",
    "# # Remove rows with non-numeric latitudes or longitudes\n",
    "# CA_data = clean_data(CA_data[['occurrenceID','stateProvince','decimalLatitude', 'decimalLongitude']].copy())\n",
    "# CA_data_gdf = gpd.GeoDataFrame(CA_data, geometry='geometry')\n",
    "\n",
    "# CA_data_gdf.set_crs(grid_within_ca.crs, inplace=True)\n",
    "# observations_in_cells = gpd.sjoin(CA_data_gdf, grid_within_ca, how='inner', predicate='within')\n",
    "\n",
    "# # Calculate the number of cells with at least one observation\n",
    "# num_cells_with_observations = observations_in_cells['index_right'].nunique()\n",
    "# observations_per_cell = observations_in_cells.groupby('index_right').size().reset_index(name='observations_count').sort_values('observations_count', ascending = False)\n",
    "\n",
    "# print(f\"Occurrences: {CA_data.shape[0]}\\nSatellite images: {num_cells_with_observations}\\nDescriptions per image {np.mean(observations_per_cell.observations_count)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obsdep",
   "language": "python",
   "name": "obsdep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
