{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Parallelize cluster_approach over states\n",
    "- Parallelize grid_approach over states\n",
    "- Make two different .py files: cluster approach, grid approach.\n",
    "- Make into folder: generating 'bounding_boxes'\n",
    "\n",
    "- Make a separate prep_herb.py\n",
    "\n",
    "- Correct eps size into haversine distance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_329770/903731494.py:2: DtypeWarning: Columns (49,56,57,59,60,61,63,64,65,66,67,68,73,74,77,78,79,83,89,91,92,95,97,98,99,100,101,102,103,105,106,107,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,135,137,138,140,141,142,143,144,145,146,147,149,150,151,152,153,154,155,160,162,163,164,167,168,171,173,174,177,178,182,183,184,185,186,189,190,191,192,193,194,195,196,197,198,199,200,204,208,209,210,211,220,221) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  occ = pd.read_csv(\"/data/cher/universe7/herbarium/data/MO-herbarium/occurrence.txt\", sep=\"\\t\", on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "# Get Herbarium data. Find some candidate locations.\n",
    "occ = pd.read_csv(\"/data/cher/universe7/herbarium/data/MO-herbarium/occurrence.txt\", sep=\"\\t\", on_bad_lines='skip')\n",
    "occ = occ[[\n",
    "    'gbifID', 'occurrenceID', # Joining Keys\n",
    "    'speciesKey', 'species', # species\n",
    "    'year', 'month', 'day', # date\n",
    "    'habitat', 'locality', # descriptive text. I think this is most of the route / habitat info we would want. Looks like already OCR processed?\n",
    "    'countryCode', 'stateProvince', 'county', 'municipality', # administrative\n",
    "    'georeferenceSources','hasCoordinate', 'hasGeospatialIssues' ,'decimalLatitude', 'decimalLongitude']] # geospatial\n",
    "    # 'level0Gid', 'level0Name', 'level1Gid', 'level1Name', 'level2Gid', 'level2Name', 'level3Gid', 'level3Name',]] # Has NAs. Above is more reliable.\n",
    "\n",
    "habitat_info = occ[~occ['habitat'].isna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random location\n",
    "random_row = habitat_info[~habitat_info['habitat'].isna()].iloc[-4]\n",
    "lat, lon = random_row['decimalLatitude'], random_row['decimalLongitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAIP \n",
    "- Only 3 band RGB through WMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/data/cher/universe7/herbarium/data/naip/1.tiff',\n",
       " <http.client.HTTPMessage at 0x7f105f5980e0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "image_url = f\"https://basemap.nationalmap.gov/arcgis/services/USGSImageryOnly/MapServer/WMSServer?service=WMS&version=1.1.1&request=GetMap&layers=0&styles=&width=256&height=256&srs=EPSG:4326&bbox={lon-0.005},{lat-0.005},{lon+0.005},{lat+0.005}&format=image/png\"\n",
    "out_file = f'/data/cher/universe7/herbarium/data/naip/1.png'\n",
    "\n",
    "urllib.request.urlretrieve(image_url, out_file)\n",
    "\n",
    "with rasterio.open(out_file) as src:\n",
    "    # Get the number of bands\n",
    "    num_bands = src.count\n",
    "    print(f'The TIFF file has {num_bands} bands.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentinel2\n",
    "- Create csv file to push into CVGlobal \n",
    "    1) Cluster points state-by-state using DBSCAN into max areas of 512 x 512 meters --> cell\n",
    "    2) Find center point of each cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, box\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) DBSCAN still generates clustered grid cells outside of 512 meters.\n",
    "- Instead we will do the following --> \n",
    "    1) generate DBSCAN clusters. Find points within those centers. \n",
    "    2) Points outside centers --> DBSCAN. Find points within those centers.\n",
    "\n",
    "2) Fast --> Create grid for state. Find overlap of points. Return just those grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_cluster\n",
      "Michigan    788\n",
      "Name: count, dtype: int64 state_cluster\n",
      "Michigan    4479\n",
      "Name: count, dtype: int64 state_cluster\n",
      "Michigan    5.68401\n",
      "Name: count, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "state_cluster\n",
       "Michigan    5.68401\n",
       "Name: count, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get general info.\n",
    "num_bbox_per_state = clusters['state_cluster'].str.split('_').str[0].value_counts()\n",
    "num_images_per_state = cluster_assignments['state_cluster'].str.split('_').str[0].value_counts()\n",
    "num_images_per_cluster = num_images_per_state / num_bbox_per_state\n",
    "\n",
    "print(num_bbox_per_state, num_images_per_state, num_images_per_cluster)\n",
    "\n",
    "display(num_images_per_cluster.sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = gpd.read_file(\"/data/cher/universe7/herbarium/data/geocell/grid.geojson\")\n",
    "grid_key = pd.read_csv(\"/data/cher/universe7/herbarium/data/geocell/grid_key.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sanity checks\n",
    "# Are places assigned to the same cluster within 512 meters of one another?\n",
    "habitat_gdf_w_bbox_cluster = gdf.merge(cluster_assignments, on = ['occurrenceID'])\n",
    "\n",
    "ex = habitat_gdf_w_bbox_cluster[habitat_gdf_w_bbox_cluster['stateProvince'] == 'New York']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use grid cells across US that have occurrences for query of images.\n",
    "# sentinal2_key.csv -- Source - gbif, key - numeric_key, lat, lon  \n",
    "# obs_id2key .csv -- [Source, obs_id, key, bbox_bounds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurrences: 12654\n",
      "Satellite images: 4798\n",
      "Descriptions per image 2.634014172571905\n"
     ]
    }
   ],
   "source": [
    "# TODO: Parallelize this and loop across states --> grid.py\n",
    "#  ## Inefficient method -- Grid across state. point-in-polygon.\n",
    "    # 1) Create equal spatial grid of CA (512M x 512M)\n",
    "    # 2) Find center point of each cell\n",
    "    # 3) Remove grids outside CA\n",
    "    # 4) Calculate # of cells with observation\n",
    "    # 5) Calculate # of observations per cell\n",
    "\n",
    "# # California ~4 min\n",
    "\n",
    "# ## Inefficient method\n",
    "# import geopandas as gpd\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from shapely.geometry import box, Point\n",
    "# from joblib import Parallel, delayed\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def build_grid(cell_size_m, bbox):\n",
    "#     # Approximate conversion factors\n",
    "#     deg_per_meter_lat = 1 / 111000  # degrees per meter for latitude\n",
    "#     deg_per_meter_lon = 1 / (111000 * np.cos(np.radians((bbox[1] + bbox[3]) / 2)))  # degrees per meter for longitude\n",
    "\n",
    "#     # Calculate the number of rows and columns\n",
    "#     lat_range = bbox[3] - bbox[1]  # degrees of latitude\n",
    "#     lon_range = bbox[2] - bbox[0]  # degrees of longitude\n",
    "\n",
    "#     nrows = int(np.ceil(lat_range / (cell_size_m * deg_per_meter_lat)))\n",
    "#     ncols = int(np.ceil(lon_range / (cell_size_m * deg_per_meter_lon)))\n",
    "\n",
    "#     # Use tqdm to create a progress bar\n",
    "#     tasks = [(i, j, cell_size_m, deg_per_meter_lat, deg_per_meter_lon, bbox) for i in range(nrows) for j in range(ncols)]\n",
    "#     with Parallel(n_jobs=-1) as parallel:\n",
    "#         grid_polygons = list(tqdm(parallel(delayed(create_cell)(*task) for task in tasks), total=len(tasks)))\n",
    "\n",
    "#     return grid_polygons\n",
    "\n",
    "# def create_cell(i, j, cell_size_m, deg_per_meter_lat, deg_per_meter_lon, bbox):\n",
    "#     # Calculate the corners of the cell\n",
    "#     min_lat = bbox[1] + i * cell_size_m * deg_per_meter_lat\n",
    "#     max_lat = bbox[1] + (i + 1) * cell_size_m * deg_per_meter_lat\n",
    "#     min_lon = bbox[0] + j * cell_size_m * deg_per_meter_lon\n",
    "#     max_lon = bbox[0] + (j + 1) * cell_size_m * deg_per_meter_lon\n",
    "\n",
    "#     # Create a polygon for the cell\n",
    "#     return box(min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "# california = gpd.read_file('/data/cher/universe7/herbarium/data/California_State_Boundary.geojson')\n",
    "# grid_polygons = build_grid(cell_size_m = 1024, bbox = california.total_bounds)\n",
    "\n",
    "# # Create a GeoDataFrame for the grid\n",
    "# grid_gdf = gpd.GeoDataFrame(geometry=grid_polygons)\n",
    "# grid_gdf.set_crs(california.crs, inplace=True)\n",
    "# grid_within_ca = gpd.sjoin(grid_gdf, california, how='inner', predicate='intersects')\n",
    "\n",
    "# # Create a DataFrame with center points of remaining cells\n",
    "# grid_within_ca = gpd.GeoDataFrame(grid_within_ca[['geometry']], geometry='geometry')\n",
    "# grid_centers = grid_within_ca.geometry.apply(lambda geom: geom.centroid)\n",
    "# centroid_points = pd.DataFrame(grid_centers.apply(lambda point: (point.x, point.y)).tolist(), columns=['Longitude', 'Latitude'])\n",
    "\n",
    "# CA_data = habitat_info[habitat_info['stateProvince'] == 'California']\n",
    "# # Remove rows with non-numeric latitudes or longitudes\n",
    "# CA_data = clean_data(CA_data[['occurrenceID','stateProvince','decimalLatitude', 'decimalLongitude']].copy())\n",
    "# CA_data_gdf = gpd.GeoDataFrame(CA_data, geometry='geometry')\n",
    "\n",
    "# CA_data_gdf.set_crs(grid_within_ca.crs, inplace=True)\n",
    "# observations_in_cells = gpd.sjoin(CA_data_gdf, grid_within_ca, how='inner', predicate='within')\n",
    "\n",
    "# # Calculate the number of cells with at least one observation\n",
    "# num_cells_with_observations = observations_in_cells['index_right'].nunique()\n",
    "# observations_per_cell = observations_in_cells.groupby('index_right').size().reset_index(name='observations_count').sort_values('observations_count', ascending = False)\n",
    "\n",
    "# print(f\"Occurrences: {CA_data.shape[0]}\\nSatellite images: {num_cells_with_observations}\\nDescriptions per image {np.mean(observations_per_cell.observations_count)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obsdep",
   "language": "python",
   "name": "obsdep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
